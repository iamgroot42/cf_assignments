{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.platform import flags\n",
    "import parser \n",
    "\n",
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "# flags.DEFINE_string('dataset_folder', './ml-100k/', 'Path to Movielens 100k/1m dataset')\n",
    "# flags.DEFINE_integer('bsize', 32, 'Batch size')\n",
    "# flags.DEFINE_integer('num_iters', 500, 'Number of iterations')\n",
    "# flags.DEFINE_integer('show_every', 10, 'Show trainign and testing accuracy after every X iterations')\n",
    "# flags.DEFINE_float('lr', 0.01, 'Learning rate')\n",
    "# flags.DEFINE_float('reg_lambda', 0.01, \"Lambda for regularization\")\n",
    "\n",
    "# Training Parameters\n",
    "# learning_rate = FLAGS.lr\n",
    "# num_iters = FLAGS.num_iters\n",
    "# batch_size = FLAGS.bsize\n",
    "# lamb = FLAGS.reg_lambda\n",
    "# dd = FLAGS.dataset_folder\n",
    "# show_every = FLAGS.show_every\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_iters = 20000\n",
    "batch_size = 16\n",
    "lamb = 0.1\n",
    "dd = './ml-100k/'\n",
    "show_every = 100\n",
    "\n",
    "\n",
    "# Don't hog GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nmae(y, y_, minr=1, maxr=5):\n",
    "    flaty = y.flatten()\n",
    "    flaty_ = y_.flatten()\n",
    "    relevant_count = flaty.shape[0]\n",
    "    return np.sum(np.absolute(flaty-flaty_))/(relevant_count * (maxr-minr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autorec(num_input, num_hidden, data_generator, test_data):\n",
    "    X = tf.placeholder(\"float\", [None, R.shape[1]])\n",
    "    \n",
    "    weights = {\n",
    "        'encoder': tf.Variable(tf.random_normal([num_input, num_hidden])),\n",
    "        'decoder': tf.Variable(tf.random_normal([num_hidden, num_input])),\n",
    "    }\n",
    "    biases = {\n",
    "        'encoder': tf.Variable(tf.random_normal([num_hidden])),\n",
    "        'decoder': tf.Variable(tf.random_normal([num_input])),\n",
    "    }\n",
    "\n",
    "    def encoder(x):\n",
    "        layer = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder']), biases['encoder']))\n",
    "        mask = tf.cast(tf.not_equal(tf.constant(0, dtype=tf.float32), x), tf.float32)\n",
    "        return layer, mask\n",
    "\n",
    "    def decoder(x, mask):\n",
    "        layer = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder']), biases['decoder']))\n",
    "        masked_backfunc = tf.multiply(layer, mask)\n",
    "        # Trick to allow back-gradient propagataion for only part of input nodes\n",
    "        back_masked_layer = masked_backfunc + tf.stop_gradient(layer - masked_backfunc) \n",
    "        return layer\n",
    "    \n",
    "    encoder_op, mask = encoder(X)\n",
    "    decoder_op = decoder(encoder_op, mask)\n",
    "\n",
    "    y_pred = decoder_op\n",
    "    y_true = X\n",
    "    \n",
    "    # Normal MSE loss for autoencoder:\n",
    "    loss = tf.reduce_mean(tf.where(y_true==0, tf.zeros_like(y_true), tf.pow(y_true - y_pred, 2)))\n",
    "    # Regularization term:\n",
    "    loss += (lamb/2) * (tf.nn.l2_loss(weights['encoder']) + tf.nn.l2_loss(weights['decoder']))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()    \n",
    "    sess.run(init)\n",
    "    \n",
    "    # Train model\n",
    "    for i in range(num_iters):\n",
    "        batch_x = data_generator.next()\n",
    "        _, l = sess.run([optimizer, loss], feed_dict={X: batch_x})\n",
    "        if i%show_every == 0:\n",
    "            print(\"Training loss (MSE) : %.4f\" % (l))\n",
    "    \n",
    "    # Test model and report NMAE\n",
    "    test_data_pred = sess.run(decoder_op, feed_dict={X: test_data})\n",
    "    return nmae(test_data, test_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (MSE) : 8419.6084\n",
      "Training loss (MSE) : 7618.3550\n",
      "Training loss (MSE) : 6893.1523\n",
      "Training loss (MSE) : 6237.1587\n",
      "Training loss (MSE) : 5643.6250\n",
      "Training loss (MSE) : 5106.8501\n",
      "Training loss (MSE) : 4620.1230\n",
      "Training loss (MSE) : 4180.6426\n",
      "Training loss (MSE) : 3783.2258\n",
      "Training loss (MSE) : 3422.7029\n",
      "Training loss (MSE) : 3097.1973\n",
      "Training loss (MSE) : 2802.4688\n",
      "Training loss (MSE) : 2535.7634\n",
      "Training loss (MSE) : 2294.1951\n",
      "Training loss (MSE) : 2076.0732\n",
      "Training loss (MSE) : 1878.7706\n",
      "Training loss (MSE) : 1699.6700\n",
      "Training loss (MSE) : 1538.0349\n",
      "Training loss (MSE) : 1391.9501\n",
      "Training loss (MSE) : 1259.2211\n",
      "Training loss (MSE) : 1139.6647\n",
      "Training loss (MSE) : 1031.1251\n",
      "Training loss (MSE) : 933.0428\n",
      "Training loss (MSE) : nan\n",
      "Training loss (MSE) : 764.4184\n",
      "Training loss (MSE) : 691.2404\n",
      "Training loss (MSE) : 625.3925\n",
      "Training loss (MSE) : 566.0485\n",
      "Training loss (MSE) : 512.3647\n",
      "Training loss (MSE) : 463.5489\n",
      "Training loss (MSE) : 419.6977\n",
      "Training loss (MSE) : 379.7954\n",
      "Training loss (MSE) : 343.5060\n",
      "Training loss (MSE) : 311.5704\n",
      "Training loss (MSE) : 281.5173\n",
      "Training loss (MSE) : 254.8296\n",
      "Training loss (MSE) : 230.3874\n",
      "Training loss (MSE) : 208.6233\n",
      "Training loss (MSE) : 189.1091\n",
      "Training loss (MSE) : 170.7381\n",
      "Training loss (MSE) : 154.8760\n",
      "Training loss (MSE) : 140.3770\n",
      "Training loss (MSE) : 126.7228\n",
      "Training loss (MSE) : 115.1423\n",
      "Training loss (MSE) : 104.1211\n",
      "Training loss (MSE) : 94.0999\n",
      "Training loss (MSE) : 85.3358\n",
      "Training loss (MSE) : 77.2525\n",
      "Training loss (MSE) : 69.9936\n",
      "Training loss (MSE) : 63.3244\n",
      "Training loss (MSE) : 57.6578\n",
      "Training loss (MSE) : 52.0406\n",
      "Training loss (MSE) : 46.9147\n",
      "Training loss (MSE) : 42.8294\n",
      "Training loss (MSE) : 38.9661\n",
      "Training loss (MSE) : 34.9687\n",
      "Training loss (MSE) : 31.9584\n",
      "Training loss (MSE) : 29.0386\n",
      "Training loss (MSE) : 26.3377\n",
      "Training loss (MSE) : 23.8865\n",
      "Training loss (MSE) : 21.7641\n",
      "Training loss (MSE) : 19.6387\n",
      "Training loss (MSE) : 17.8986\n",
      "Training loss (MSE) : 16.3356\n",
      "Training loss (MSE) : 15.1819\n",
      "Training loss (MSE) : 13.1017\n",
      "Training loss (MSE) : 12.1373\n",
      "Training loss (MSE) : 11.4905\n",
      "Training loss (MSE) : 9.9834\n",
      "Training loss (MSE) : 9.3072\n",
      "Training loss (MSE) : 8.4934\n",
      "Training loss (MSE) : 7.7310\n",
      "Training loss (MSE) : 6.7926\n",
      "Training loss (MSE) : 6.3944\n",
      "Training loss (MSE) : 6.0915\n",
      "Training loss (MSE) : 5.2370\n",
      "Training loss (MSE) : 4.8862\n",
      "Training loss (MSE) : 4.7292\n",
      "Training loss (MSE) : 4.0476\n",
      "Training loss (MSE) : 3.9604\n",
      "Training loss (MSE) : 3.5286\n",
      "Training loss (MSE) : 3.2596\n",
      "Training loss (MSE) : nan\n",
      "Training loss (MSE) : 3.2123\n",
      "Training loss (MSE) : 2.4949\n",
      "Training loss (MSE) : 2.2067\n",
      "Training loss (MSE) : 2.1774\n",
      "Training loss (MSE) : 2.1678\n",
      "Training loss (MSE) : 1.9200\n",
      "Training loss (MSE) : 2.0075\n",
      "Training loss (MSE) : 1.8639\n",
      "Training loss (MSE) : 1.5555\n",
      "Training loss (MSE) : 2.1577\n",
      "Training loss (MSE) : 1.5647\n",
      "Training loss (MSE) : 1.5284\n",
      "Training loss (MSE) : 1.1987\n",
      "Training loss (MSE) : 1.2517\n",
      "Training loss (MSE) : 1.4767\n",
      "Training loss (MSE) : 0.9693\n",
      "Training loss (MSE) : 1.2622\n",
      "Training loss (MSE) : 1.3894\n",
      "Training loss (MSE) : 0.9670\n",
      "Training loss (MSE) : 1.3557\n",
      "Training loss (MSE) : 1.1661\n",
      "Training loss (MSE) : 0.9481\n",
      "Training loss (MSE) : 1.0472\n",
      "Training loss (MSE) : 0.9894\n",
      "Training loss (MSE) : 0.9929\n",
      "Training loss (MSE) : 0.8913\n",
      "Training loss (MSE) : 1.1651\n",
      "Training loss (MSE) : 0.9266\n",
      "Training loss (MSE) : 0.6681\n",
      "Training loss (MSE) : 0.9850\n",
      "Training loss (MSE) : 1.1026\n",
      "Training loss (MSE) : 0.7122\n",
      "Training loss (MSE) : 0.9623\n",
      "Training loss (MSE) : 0.9911\n",
      "Training loss (MSE) : 0.9608\n",
      "Training loss (MSE) : 0.9271\n",
      "Training loss (MSE) : 0.9889\n",
      "Training loss (MSE) : 0.8410\n",
      "Training loss (MSE) : 0.8908\n",
      "Training loss (MSE) : 0.9465\n",
      "Training loss (MSE) : 1.2563\n",
      "Training loss (MSE) : 0.5033\n",
      "Training loss (MSE) : 0.7377\n",
      "Training loss (MSE) : 1.1753\n",
      "Training loss (MSE) : 0.6507\n",
      "Training loss (MSE) : 0.8624\n",
      "Training loss (MSE) : 0.8519\n",
      "Training loss (MSE) : 0.8169\n",
      "Training loss (MSE) : 0.5372\n",
      "Training loss (MSE) : 0.7341\n",
      "Training loss (MSE) : 0.9698\n",
      "Training loss (MSE) : 0.6028\n",
      "Training loss (MSE) : 0.6930\n",
      "Training loss (MSE) : 0.9347\n",
      "Training loss (MSE) : 0.6147\n",
      "Training loss (MSE) : 0.8537\n",
      "Training loss (MSE) : 0.7180\n",
      "Training loss (MSE) : 0.7164\n",
      "Training loss (MSE) : nan\n",
      "Training loss (MSE) : 1.1294\n",
      "Training loss (MSE) : 0.6107\n",
      "Training loss (MSE) : 0.5017\n",
      "Training loss (MSE) : 0.6344\n",
      "Training loss (MSE) : 0.7715\n",
      "Training loss (MSE) : 0.6568\n",
      "Training loss (MSE) : 0.8641\n",
      "Training loss (MSE) : 0.8291\n",
      "Training loss (MSE) : 0.6194\n",
      "Training loss (MSE) : 1.3099\n",
      "Training loss (MSE) : 0.7978\n",
      "Training loss (MSE) : 0.8347\n",
      "Training loss (MSE) : 0.5709\n",
      "Training loss (MSE) : 0.6835\n",
      "Training loss (MSE) : 0.9624\n",
      "Training loss (MSE) : 0.5041\n",
      "Training loss (MSE) : 0.8407\n",
      "Training loss (MSE) : 1.0078\n",
      "Training loss (MSE) : 0.6220\n",
      "Training loss (MSE) : 1.0432\n",
      "Training loss (MSE) : 0.8830\n",
      "Training loss (MSE) : 0.6922\n",
      "Training loss (MSE) : 0.8153\n",
      "Training loss (MSE) : 0.7794\n",
      "Training loss (MSE) : 0.8030\n",
      "Training loss (MSE) : 0.7193\n",
      "Training loss (MSE) : 1.0092\n",
      "Training loss (MSE) : 0.7855\n",
      "Training loss (MSE) : 0.5406\n",
      "Training loss (MSE) : 0.8693\n",
      "Training loss (MSE) : 0.9976\n",
      "Training loss (MSE) : 0.6173\n",
      "Training loss (MSE) : 0.8763\n",
      "Training loss (MSE) : 0.9131\n",
      "Training loss (MSE) : 0.8901\n",
      "Training loss (MSE) : 0.8632\n",
      "Training loss (MSE) : 0.9307\n",
      "Training loss (MSE) : 0.7883\n",
      "Training loss (MSE) : 0.8430\n",
      "Training loss (MSE) : 0.9030\n",
      "Training loss (MSE) : 1.2169\n",
      "Training loss (MSE) : 0.4678\n",
      "Training loss (MSE) : 0.7053\n",
      "Training loss (MSE) : 1.1458\n",
      "Training loss (MSE) : 0.6240\n",
      "Training loss (MSE) : 0.8380\n",
      "Training loss (MSE) : 0.8297\n",
      "Training loss (MSE) : 0.7968\n",
      "Training loss (MSE) : 0.5189\n",
      "Training loss (MSE) : 0.7173\n",
      "Training loss (MSE) : 0.9546\n",
      "Training loss (MSE) : 0.5889\n",
      "Training loss (MSE) : 0.6803\n",
      "Training loss (MSE) : 0.9230\n",
      "Training loss (MSE) : 0.6042\n",
      "Training loss (MSE) : 0.8440\n",
      "Training loss (MSE) : 0.7091\n",
      "Training loss (MSE) : 0.7083\n",
      "NMAE for this validation: 0.631455\n"
     ]
    }
   ],
   "source": [
    "movies = parser.load_data(dd + \"u.item\", parser.Movie, '|')\n",
    "ratings = []\n",
    "\n",
    "for i in range(1,1+5):\n",
    "    ratings.append( [parser.load_data(dd + \"u\" + str(i) + \".base\", parser.Rating, '\\t'), parser.load_data(dd + \"u\" + str(i) + \".test\", parser.Rating, '\\t')] )\n",
    "users = parser.load_data(dd + \"u.user\", parser.User, '|')\n",
    "\n",
    "errors = []\n",
    "for fold in ratings:\n",
    "    R = parser.R_matrix(len(users), len(movies), fold[0]).astype('float32')\n",
    "    R_test = parser.R_matrix(len(users), len(movies), fold[1]).astype('float32')\n",
    "    \n",
    "    # Fill missing entries with 3, as done in the paper\n",
    "    R_test += (R_test==0)*3\n",
    "    \n",
    "    data_generator = parser.get_next_batch(R, batch_size)\n",
    "    num_hidden = 1\n",
    "    #[10, 20, 40, 80, 100, 200, 300, 400, 500]\n",
    "    \n",
    "    error = autorec(R.shape[1], 100, data_generator, R_test)\n",
    "    print(\"NMAE for this validation: %f\" % (error))\n",
    "    errors.append(error)\n",
    "    break\n",
    "# print(\"NMAE for all folds: %f\" % (sum(errors)/float(len(errors))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
